## NLP-Disaster-Tweets

![Screen Shot 2022-10-24 at 12 41 08 PM](https://user-images.githubusercontent.com/90480106/197590965-aec1f4e1-a249-44ce-9409-7a4976163910.png)

Kaggle link: https://www.kaggle.com/competitions/nlp-getting-started

Dataset:

  * Columns: Location, Keyword, Tweets

Apply LSTM, BERT, Random Forest, and Naive Bayes model to classify disaster tweets.

![Screen Shot 2022-10-24 at 1 31 07 PM](https://user-images.githubusercontent.com/90480106/197599372-3ae8175e-88cc-4ff3-9041-99c30b93b224.png)

![Screen Shot 2022-10-24 at 1 34 23 PM](https://user-images.githubusercontent.com/90480106/197599924-d69e0ab2-03f7-4b2e-83a9-a4f0abe2f74a.png)

NLP_Disaster_Tweets_2.ipynb: All the code including LSTM, BERT, Random Forest, and Naive Bayes

Disaster_Tweets_LSTM: Clear LSTM model, scoring 0.79865.

Disaster_Tweets_BERT: Clear BERT model, scoring 0.82899.

Disaster_Tweets_NB: Clear Naive Bayes model, scoring 0.78823.

Score: 0.82899 (109/731)

Keep working on fine tuning BERT using sentence transformers!
